{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1bd82a",
   "metadata": {},
   "source": [
    "# Montney: MICE (max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47baab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeda364",
   "metadata": {},
   "source": [
    "## 1. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534b988d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WELL_ID</th>\n",
       "      <th>DISPLAY_NAME</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>GR</th>\n",
       "      <th>SONIC</th>\n",
       "      <th>RESISTIVITY</th>\n",
       "      <th>SP</th>\n",
       "      <th>POROSITY</th>\n",
       "      <th>CALIPER</th>\n",
       "      <th>BITSIZE</th>\n",
       "      <th>TOC_CORE</th>\n",
       "      <th>TOC_CUTTINGS</th>\n",
       "      <th>LITHO</th>\n",
       "      <th>ORDER_3</th>\n",
       "      <th>ORDER_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100031508122W500</td>\n",
       "      <td>0/03-15-081-22W5</td>\n",
       "      <td>2001</td>\n",
       "      <td>477579.55677</td>\n",
       "      <td>6.207776e+06</td>\n",
       "      <td>825.620</td>\n",
       "      <td>2.477900</td>\n",
       "      <td>81.453</td>\n",
       "      <td>255.236</td>\n",
       "      <td>4.782</td>\n",
       "      <td>-20.340</td>\n",
       "      <td>21.909</td>\n",
       "      <td>164.721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MONTNEY FM</td>\n",
       "      <td>Sequence 2</td>\n",
       "      <td>LST2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100031508122W500</td>\n",
       "      <td>0/03-15-081-22W5</td>\n",
       "      <td>2001</td>\n",
       "      <td>477579.55677</td>\n",
       "      <td>6.207776e+06</td>\n",
       "      <td>825.745</td>\n",
       "      <td>2.496034</td>\n",
       "      <td>85.738</td>\n",
       "      <td>255.687</td>\n",
       "      <td>4.793</td>\n",
       "      <td>-18.998</td>\n",
       "      <td>21.502</td>\n",
       "      <td>165.051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MONTNEY FM</td>\n",
       "      <td>Sequence 1</td>\n",
       "      <td>TST1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100031508122W500</td>\n",
       "      <td>0/03-15-081-22W5</td>\n",
       "      <td>2001</td>\n",
       "      <td>477579.55677</td>\n",
       "      <td>6.207776e+06</td>\n",
       "      <td>825.870</td>\n",
       "      <td>2.522904</td>\n",
       "      <td>93.034</td>\n",
       "      <td>258.596</td>\n",
       "      <td>4.499</td>\n",
       "      <td>-15.166</td>\n",
       "      <td>21.561</td>\n",
       "      <td>164.248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MONTNEY FM</td>\n",
       "      <td>Sequence 1</td>\n",
       "      <td>TST1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100031508122W500</td>\n",
       "      <td>0/03-15-081-22W5</td>\n",
       "      <td>2001</td>\n",
       "      <td>477579.55677</td>\n",
       "      <td>6.207776e+06</td>\n",
       "      <td>825.995</td>\n",
       "      <td>2.548484</td>\n",
       "      <td>102.140</td>\n",
       "      <td>264.808</td>\n",
       "      <td>4.126</td>\n",
       "      <td>-8.029</td>\n",
       "      <td>22.043</td>\n",
       "      <td>163.497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MONTNEY FM</td>\n",
       "      <td>Sequence 1</td>\n",
       "      <td>TST1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100031508122W500</td>\n",
       "      <td>0/03-15-081-22W5</td>\n",
       "      <td>2001</td>\n",
       "      <td>477579.55677</td>\n",
       "      <td>6.207776e+06</td>\n",
       "      <td>826.120</td>\n",
       "      <td>2.566674</td>\n",
       "      <td>111.973</td>\n",
       "      <td>273.186</td>\n",
       "      <td>3.567</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>23.000</td>\n",
       "      <td>165.123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MONTNEY FM</td>\n",
       "      <td>Sequence 1</td>\n",
       "      <td>TST1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            WELL_ID      DISPLAY_NAME  YEAR             X             Y  \\\n",
       "0  100031508122W500  0/03-15-081-22W5  2001  477579.55677  6.207776e+06   \n",
       "1  100031508122W500  0/03-15-081-22W5  2001  477579.55677  6.207776e+06   \n",
       "2  100031508122W500  0/03-15-081-22W5  2001  477579.55677  6.207776e+06   \n",
       "3  100031508122W500  0/03-15-081-22W5  2001  477579.55677  6.207776e+06   \n",
       "4  100031508122W500  0/03-15-081-22W5  2001  477579.55677  6.207776e+06   \n",
       "\n",
       "     DEPTH   DENSITY       GR    SONIC  RESISTIVITY      SP  POROSITY  \\\n",
       "0  825.620  2.477900   81.453  255.236        4.782 -20.340    21.909   \n",
       "1  825.745  2.496034   85.738  255.687        4.793 -18.998    21.502   \n",
       "2  825.870  2.522904   93.034  258.596        4.499 -15.166    21.561   \n",
       "3  825.995  2.548484  102.140  264.808        4.126  -8.029    22.043   \n",
       "4  826.120  2.566674  111.973  273.186        3.567  -0.953    23.000   \n",
       "\n",
       "   CALIPER  BITSIZE  TOC_CORE  TOC_CUTTINGS       LITHO     ORDER_3 ORDER_4  \n",
       "0  164.721      NaN       NaN           NaN  MONTNEY FM  Sequence 2    LST2  \n",
       "1  165.051      NaN       NaN           NaN  MONTNEY FM  Sequence 1    TST1  \n",
       "2  164.248      NaN       NaN           NaN  MONTNEY FM  Sequence 1    TST1  \n",
       "3  163.497      NaN       NaN           NaN  MONTNEY FM  Sequence 1    TST1  \n",
       "4  165.123      NaN       NaN           NaN  MONTNEY FM  Sequence 1    TST1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# montney dataset\n",
    "montney = pd.read_csv(filepath_or_buffer='dataset_montney.csv', low_memory=False)\n",
    "montney.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839ca7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "montney.rename(columns={'DENSITY': 'RHOB', \n",
    "                        'SONIC': 'DT',\n",
    "                        'RESISTIVITY': 'RES', \n",
    "                        'POROSITY': 'NPHI',\n",
    "                        'ORDER_4': 'STRAT'\n",
    "                       }, inplace=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91da72a6-3a69-42e6-9195-e9db28c2ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "montney.drop(['DISPLAY_NAME', 'CALIPER', 'BITSIZE', 'TOC_CORE', 'TOC_CUTTINGS', 'ORDER_3'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace sonic values smaller than 0 for NaN values\n",
    "mask = montney['DT'] < 0\n",
    "montney.loc[mask, 'DT'] = np.nan\n",
    "\n",
    "# replace resistivity values smaller than or equal to 0 for NaN values\n",
    "mask = montney['RES'] <= 0\n",
    "montney.loc[mask, 'RES'] = np.nan\n",
    "\n",
    "# create a new column to store log base 10 of resistivity\n",
    "montney['RES_10'] = np.log10(montney['RES']+1)\n",
    "\n",
    "# replace neutron porosity values smaller than 0 for NaN values\n",
    "mask = montney['NPHI'] < 0\n",
    "montney.loc[mask, 'NPHI'] = np.nan\n",
    "\n",
    "# convert percentage to fraction\n",
    "def convert_neutron(x):\n",
    "    return x / 100\n",
    "\n",
    "montney['NPHI'] = montney['NPHI'].apply(convert_neutron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b852f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding for well id\n",
    "well_encoder = LabelEncoder()\n",
    "montney['WELL'] = well_encoder.fit_transform(montney['WELL_ID'])\n",
    "\n",
    "# label encoding for stratigraphy\n",
    "strat_encoder = LabelEncoder()\n",
    "montney['STRAT_ENCODED'] = strat_encoder.fit_transform(montney['STRAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d34f308-da10-4952-9c97-6b2469a88b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WELL_ID', 'YEAR', 'X', 'Y', 'DEPTH', 'RHOB', 'GR', 'DT', 'RES', 'SP',\n",
       "       'NPHI', 'LITHO', 'STRAT', 'RES_10', 'WELL', 'STRAT_ENCODED'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns\n",
    "montney.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e3010c-55c4-43d3-bfad-761dbff0d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "selected_features = ['WELL', 'X', 'Y', 'DEPTH', \n",
    "                     'RHOB', 'GR', 'DT', 'RES_10', 'SP', 'NPHI',\n",
    "                     'STRAT_ENCODED']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8e4c5-5dec-49a3-a73a-89be0a31082d",
   "metadata": {},
   "source": [
    "## 2. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3065abe7-9c02-43e9-8967-6bb9af33b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of the dataset for modeling\n",
    "data_ml = montney[selected_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "466ee954-1ea4-4b3d-9d09-eefbfbd5daf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of data in train set: 0.85\n",
      "Fraction of data in test set: 0.15\n",
      "Total number of samples in dataset: 122374\n"
     ]
    }
   ],
   "source": [
    "# well test selection\n",
    "test_wells = well_encoder.transform(['100042408423W600', '100041606522W500', '100071406522W500', \n",
    "                                     '100111408125W500', '100143207308W600', '100162407706W600'])\n",
    "\n",
    "# mask for test well\n",
    "test_mask = data_ml['WELL'].isin(test_wells)\n",
    "\n",
    "# column to identify train and test wells\n",
    "data_ml['train_test'] = 'Train'\n",
    "data_ml.loc[test_mask, 'train_test'] = 'Test'\n",
    "\n",
    "# fraction of data\n",
    "train_fraction = data_ml[data_ml['train_test'] == 'Train'].shape[0] / data_ml.shape[0]\n",
    "test_fraction = data_ml[data_ml['train_test'] == 'Test'].shape[0] / data_ml.shape[0]\n",
    "print(f\"Fraction of data in train set: {train_fraction:.2f}\")\n",
    "print(f\"Fraction of data in test set: {test_fraction:.2f}\")\n",
    "print(f\"Total number of samples in dataset: {data_ml.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f52422d5-f455-4635-ac53-8a528c8b55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets\n",
    "train = data_ml[~test_mask].copy()\n",
    "test = data_ml[test_mask].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea21775-9fe1-493f-9447-33a056e668e0",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1975a8-9453-4e4a-b95b-7fa5ef609846",
   "metadata": {},
   "source": [
    "### 3.1. Input Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6dcca82-9c07-4940-908a-458d73685774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of train and test sets\n",
    "X_train = train.copy()\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a8dce30-6a68-4c98-ad86-a90762e2b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features to impute \n",
    "features_mice = ['RHOB', 'GR', 'DT', 'RES_10', 'SP', 'NPHI', 'STRAT_ENCODED']\n",
    "imputed_cols = ['RHOB', 'GR', 'DT', 'RES_10', 'SP', 'NPHI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12967b0b-2ed3-4af7-b7bc-4565c63b12f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Combinations: 76\n"
     ]
    }
   ],
   "source": [
    "# list with all combinations of well-logs for each well\n",
    "unique_wells = X_train['WELL'].unique()\n",
    "combinations = []\n",
    "for well in unique_wells:\n",
    "    for feature in imputed_cols:\n",
    "        # only for well-logs that are not completely NaN\n",
    "        if not X_train.loc[X_train['WELL'] == well, feature].isna().all():\n",
    "            combinations.append((well, feature))\n",
    "            \n",
    "print('Number of Combinations:', len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969c550d-504c-43d6-a06d-e1885491963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to impute NaN values using iterative imputer (MICE)\n",
    "def impute(train_data, cols_imp, model):\n",
    "    mice = IterativeImputer(estimator=model, initial_strategy='mean' , random_state=17, max_iter=20, tol=0.01)\n",
    "    mice.fit(train_data[cols_imp])\n",
    "    imputed_train = mice.transform(train_data[cols_imp])\n",
    "    return imputed_train, mice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827b83f",
   "metadata": {},
   "source": [
    "### 3.2. Model Training with the best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d543e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(X_train, model, param_grid, well_logs, cols_imp, n_splits, n_jobs):\n",
    "    \"\"\"\n",
    "    Training Model with MICE\n",
    "    \n",
    "    Parameters\n",
    "    ----------------------------------------------------------------------------------\n",
    "        X_train: (pd.DataFrame) \n",
    "            Training data\n",
    "        \n",
    "        model: (model object for imputation)\n",
    "            Model (e.g., KNeighborsRegressor, BayesianRidge, RF, XGBoost)\n",
    "            \n",
    "        param_grid: (dict)\n",
    "            Dictionary of hyperparameters for the model\n",
    "            \n",
    "        well_logs: (list)\n",
    "            List of well logs to evaluate\n",
    "            \n",
    "        cols_imp: (list)\n",
    "            List of columns to impute\n",
    "            \n",
    "        n_splits: (int)\n",
    "            Number of cross-validation splits\n",
    "            \n",
    "        n_jobs: (int)\n",
    "            Number of jobs to run in parallel during imputation\n",
    "    \n",
    "    Returns\n",
    "    ----------------------------------------------------------------------------------\n",
    "        scaler: (object)\n",
    "            MinMaxScaler object fitted on the imputed training data\n",
    "            \n",
    "        imp_model: (object)\n",
    "            Trained imputation model\n",
    "    \"\"\"\n",
    "    # copy of the data to work with\n",
    "    data_train = X_train.copy()\n",
    "\n",
    "    # scale the data for training\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data_train[cols_imp])\n",
    "    X_training_scaled = scaler.transform(data_train[cols_imp])\n",
    "    X_training_scaled_df = pd.DataFrame(X_training_scaled, \n",
    "                                        columns=cols_imp,\n",
    "                                        index=X_train.index)\n",
    "\n",
    "    # impute NaN values using iterative imputer\n",
    "    if model == KNeighborsRegressor:\n",
    "        X_training_imp, imp_model = impute(train_data= X_training_scaled_df,  \n",
    "                                           cols_imp=cols_imp,\n",
    "                                           model=model(**param_grid, n_jobs=n_jobs))\n",
    "\n",
    "    elif model == BayesianRidge:\n",
    "        X_training_imp, imp_model = impute(train_data=X_training_scaled_df,\n",
    "                                           cols_imp=cols_imp,\n",
    "                                           model=model(**param_grid))\n",
    "\n",
    "    else:\n",
    "        X_training_imp, imp_model = impute(train_data= X_training_scaled_df,  \n",
    "                                           cols_imp=cols_imp,\n",
    "                                           model=model(**param_grid, random_state=17, n_jobs=n_jobs))\n",
    "\n",
    "    return scaler, imp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20902d7c",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "383132f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_knr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a5cabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcbae\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler_knr, imp_model_knr = training_model(X_train=X_train, \n",
    "                                           model=KNeighborsRegressor, \n",
    "                                           param_grid=param_grid_knr,\n",
    "                                           well_logs=imputed_cols,\n",
    "                                           cols_imp=features_mice, \n",
    "                                           n_splits=5,\n",
    "                                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bccf93",
   "metadata": {},
   "source": [
    "### BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "406c3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_br = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33608c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_br, imp_model_br = training_model(X_train=X_train, \n",
    "                                         model=BayesianRidge, \n",
    "                                         param_grid=param_grid_br,\n",
    "                                         well_logs=imputed_cols,\n",
    "                                         cols_imp=features_mice, \n",
    "                                         n_splits=5,\n",
    "                                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7ce9a",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beeff7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff4b79a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcbae\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler_rf, imp_model_rf = training_model(X_train=X_train, \n",
    "                                         model=RandomForestRegressor, \n",
    "                                         param_grid=param_grid_rf,\n",
    "                                         well_logs=imputed_cols,\n",
    "                                         cols_imp=features_mice, \n",
    "                                         n_splits=5,\n",
    "                                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41c52b",
   "metadata": {},
   "source": [
    "### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd80b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {'max_depth': 6, 'reg_alpha': 0, 'reg_lambda': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "410064fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcbae\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler_xgb, imp_model_xgb = training_model(X_train=X_train, \n",
    "                                           model=XGBRegressor, \n",
    "                                           param_grid=param_grid_xgb,\n",
    "                                           well_logs=imputed_cols,\n",
    "                                           cols_imp=features_mice, \n",
    "                                           n_splits=5,\n",
    "                                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f84364",
   "metadata": {},
   "source": [
    "## 4. Testing Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b260a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(X_test, scaler, imp_model, well_logs, cols_imp, n_splits):\n",
    "    \"\"\"\n",
    "    Impute Test Data using Train Model\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------------------------------------------------\n",
    "        X_test: (pd.DataFrame) \n",
    "            Test data\n",
    "        \n",
    "        scaler: (scaler object)\n",
    "            Scaler object fitted on the training data\n",
    "            \n",
    "        imp_model: (imputation model object)\n",
    "            Imputation model trained on the training data\n",
    "            \n",
    "        well_logs: (list)\n",
    "            List of well logs to evaluate\n",
    "            \n",
    "        cols_imp: (list)\n",
    "            List of columns to impute\n",
    "            \n",
    "        n_splits: (int)\n",
    "            Number of cross-validation splits\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------------------------------------------------\n",
    "        combined_df: (pd.DataFrame)\n",
    "            Combined results dataframe containing original scaled values, \n",
    "            scaled imputed values, and imputed values for each well log\n",
    "    \"\"\"\n",
    "    # scale the original test data\n",
    "    X_test_original_scale = scaler.transform(X_test[cols_imp])        \n",
    "    X_test_original_scale_df = pd.DataFrame(X_test_original_scale,\n",
    "                                      columns=cols_imp,\n",
    "                                      index=X_test.index)\n",
    "    X_test_original_scale_df['WELL'] = X_test['WELL']\n",
    "    \n",
    "    unique_wells = X_test['WELL'].unique()\n",
    "    combinations = []\n",
    "    for well in unique_wells:\n",
    "        for feature in imputed_cols:\n",
    "            # only for well-logs that are not completely NaN\n",
    "            if not X_test.loc[X_test['WELL'] == well, feature].isna().all():\n",
    "                combinations.append((well, feature))\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, random_state=17, shuffle=True)\n",
    "    \n",
    "    X_test_sc_imp_result = X_test.copy()\n",
    "    X_test_imp_result = X_test.copy()\n",
    "     \n",
    "    for i, (test_index, val_index) in enumerate(kf.split(combinations)):\n",
    "\n",
    "        # test and validations sets\n",
    "        test_combinations = [combinations[i] for i in test_index]\n",
    "        validation_combinations = [combinations[i] for i in val_index]\n",
    "\n",
    "        # copy of the data to work with\n",
    "        data_test = X_test.copy()\n",
    "        \n",
    "        # set values to NaN in the data to impute using the validation combinations\n",
    "        for well_id, feature_name in validation_combinations:\n",
    "            data_test.loc[data_test['WELL']==well_id, feature_name] = np.nan\n",
    "\n",
    "\n",
    "        # scale the test with NaN using the scaler object fitted on the training data\n",
    "        X_test_scaled = scaler.transform(data_test[cols_imp])\n",
    "        X_test_scaled_df = pd.DataFrame(X_test_scaled, \n",
    "                                        columns=cols_imp,\n",
    "                                        index=X_test.index)\n",
    "\n",
    "        # impute NaN values in the test data using the imp_model trained on the training data\n",
    "        X_test_imp = imp_model.transform(X_test_scaled_df[cols_imp])\n",
    "        X_test_imp_scaled = pd.DataFrame(X_test_imp, columns=cols_imp, index=X_test.index)\n",
    "\n",
    "        \n",
    "        # inverse transform the imputed values\n",
    "        X_test_imp_unscaled = scaler.inverse_transform(X_test_imp_scaled)\n",
    "        X_test_imp_df = pd.DataFrame(X_test_imp_unscaled, \n",
    "                                     columns=cols_imp, \n",
    "                                     index=X_test.index)\n",
    "        \n",
    "        \n",
    "        # store results of the scaled imputation using validation combinations\n",
    "        for well_id, feature_name in validation_combinations:\n",
    "            scaled_imputed_result = X_test_imp_scaled.loc[X_test['WELL']==well_id, feature_name]\n",
    "            X_test_sc_imp_result.loc[X_test['WELL']==well_id, feature_name] = scaled_imputed_result\n",
    "        \n",
    "        # store results of the imputation using validation combinations\n",
    "        for well_id, feature_name in validation_combinations:\n",
    "            imputed_result = X_test_imp_df.loc[X_test['WELL']==well_id, feature_name]\n",
    "            X_test_imp_result.loc[X_test['WELL']==well_id, feature_name] = imputed_result\n",
    "                \n",
    "    # rename columns\n",
    "    X_test_original_scale_df.rename(columns=lambda x: x + '_SCALED', inplace=True)\n",
    "    X_test_sc_imp_result.rename(columns=lambda x: x + '_IMP_SCALED', inplace=True)\n",
    "    X_test_imp_result.rename(columns=lambda x: x + '_IMP', inplace=True)\n",
    "    \n",
    "    # combine dataframes\n",
    "    combined_df = pd.concat([X_test_original_scale_df[[column + '_SCALED' for column in well_logs]],\n",
    "                             X_test_sc_imp_result[[column + '_IMP_SCALED' for column in well_logs]],\n",
    "                             X_test_imp_result[[column + '_IMP' for column in well_logs]]\n",
    "                            ], axis=1)\n",
    "\n",
    "    return  combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cc7d1",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86acf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_knr = testing_model(X_test=X_test, \n",
    "                                scaler=scaler_knr, \n",
    "                                imp_model=imp_model_knr,\n",
    "                                well_logs=imputed_cols,\n",
    "                                cols_imp=features_mice,\n",
    "                                n_splits=5\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec24e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_knr.to_csv('test_result_knr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a4138",
   "metadata": {},
   "source": [
    "### BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "385e011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_br = testing_model(X_test=X_test, \n",
    "                               scaler=scaler_br, \n",
    "                               imp_model=imp_model_br,\n",
    "                               well_logs=imputed_cols,\n",
    "                               cols_imp=features_mice,\n",
    "                               n_splits=5\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9745b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_br.to_csv('test_result_br.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0269a74",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b09e84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_rf = testing_model(X_test=X_test, \n",
    "                               scaler=scaler_rf, \n",
    "                               imp_model=imp_model_rf,\n",
    "                               well_logs=imputed_cols,\n",
    "                               cols_imp=features_mice,\n",
    "                               n_splits=5\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93f2f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_rf.to_csv('test_result_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc63f4",
   "metadata": {},
   "source": [
    "### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da1819a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_xgb = testing_model(X_test=X_test, \n",
    "                                scaler=scaler_xgb, \n",
    "                                imp_model=imp_model_xgb,\n",
    "                                well_logs=imputed_cols,\n",
    "                                cols_imp=features_mice,\n",
    "                                n_splits=5\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "495794f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_xgb.to_csv('test_result_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b8157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
