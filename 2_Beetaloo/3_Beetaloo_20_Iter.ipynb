{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16250c1e",
   "metadata": {},
   "source": [
    "# Beetaloo: MICE (max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47baab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146f011",
   "metadata": {},
   "source": [
    "## 1. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534b988d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WELL_ID</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>GR</th>\n",
       "      <th>SONIC</th>\n",
       "      <th>RESISTIVITY</th>\n",
       "      <th>SP</th>\n",
       "      <th>NEUTRON</th>\n",
       "      <th>CALIPER</th>\n",
       "      <th>BITSIZE</th>\n",
       "      <th>TOC_CORE</th>\n",
       "      <th>TOC_CUTTINGS</th>\n",
       "      <th>LITHO</th>\n",
       "      <th>ORDER_3</th>\n",
       "      <th>ORDER_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander_1</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>-15.16911</td>\n",
       "      <td>134.855921</td>\n",
       "      <td>63.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Velkerri Formation</td>\n",
       "      <td>HST-2</td>\n",
       "      <td>RST-2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander_1</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>-15.16911</td>\n",
       "      <td>134.855921</td>\n",
       "      <td>63.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Velkerri Formation</td>\n",
       "      <td>HST-2</td>\n",
       "      <td>RST-2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexander_1</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>-15.16911</td>\n",
       "      <td>134.855921</td>\n",
       "      <td>63.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Velkerri Formation</td>\n",
       "      <td>HST-2</td>\n",
       "      <td>RST-2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander_1</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>-15.16911</td>\n",
       "      <td>134.855921</td>\n",
       "      <td>63.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Velkerri Formation</td>\n",
       "      <td>HST-2</td>\n",
       "      <td>RST-2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander_1</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>-15.16911</td>\n",
       "      <td>134.855921</td>\n",
       "      <td>63.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Velkerri Formation</td>\n",
       "      <td>HST-2</td>\n",
       "      <td>RST-2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WELL_ID    YEAR  LATITUDE   LONGITUDE  DEPTH  DENSITY       GR  SONIC  \\\n",
       "0  Alexander_1  1988.0 -15.16911  134.855921   63.1      NaN  111.753    NaN   \n",
       "1  Alexander_1  1988.0 -15.16911  134.855921   63.2      NaN  108.300    NaN   \n",
       "2  Alexander_1  1988.0 -15.16911  134.855921   63.3      NaN  105.699    NaN   \n",
       "3  Alexander_1  1988.0 -15.16911  134.855921   63.4      NaN  106.119    NaN   \n",
       "4  Alexander_1  1988.0 -15.16911  134.855921   63.5      NaN  106.734    NaN   \n",
       "\n",
       "   RESISTIVITY  SP  NEUTRON  CALIPER  BITSIZE  TOC_CORE  TOC_CUTTINGS  \\\n",
       "0          NaN NaN      NaN    4.705      NaN       NaN           NaN   \n",
       "1          NaN NaN      NaN    4.701      NaN       NaN           NaN   \n",
       "2          NaN NaN      NaN    4.676      NaN       NaN           NaN   \n",
       "3          NaN NaN      NaN    4.668      NaN       NaN           NaN   \n",
       "4          NaN NaN      NaN    4.691      NaN       NaN           NaN   \n",
       "\n",
       "                LITHO ORDER_3  ORDER_4  \n",
       "0  Velkerri Formation   HST-2  RST-2.5  \n",
       "1  Velkerri Formation   HST-2  RST-2.5  \n",
       "2  Velkerri Formation   HST-2  RST-2.5  \n",
       "3  Velkerri Formation   HST-2  RST-2.5  \n",
       "4  Velkerri Formation   HST-2  RST-2.5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beetaloo dataset\n",
    "beetaloo = pd.read_csv(filepath_or_buffer='dataset_beetaloo.csv', low_memory=False)\n",
    "beetaloo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839ca7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "beetaloo.rename(columns={'DENSITY': 'RHOB', \n",
    "                         'SONIC': 'DT',\n",
    "                         'RESISTIVITY': 'RES', \n",
    "                         'NEUTRON': 'NPHI',\n",
    "                         'ORDER_4': 'STRAT'\n",
    "                        }, inplace=True\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c1fa9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the years of some wells\n",
    "wells_no_year = ['Balmain_1', 'Jamison_1', 'Lady_Penrhyn_1', 'Ronald_1', 'Scarborough_1', 'Sever_1', 'Walton_2']\n",
    "years = [1992, 1991, 1987, 1993, 1987, 1990, 1989]\n",
    "\n",
    "for i in range(len(wells_no_year)):\n",
    "    beetaloo.loc[beetaloo['WELL_ID'] == wells_no_year[i], 'YEAR'] = years[i] \n",
    "    \n",
    "    \n",
    "# complete the location of some wells\n",
    "wells_no_location = ['Balmain_1', 'Birdum_Creek_1', 'Broadmere_1', 'Jamison_1', 'Lady_Penrhyn_1', 'Marmbulligan_1', 'Ronald_1', 'Scarborough_1', 'Sever_1', 'Tarlee_1', 'Walton_2', 'Wyworrie_1']\n",
    "\n",
    "x_coord = [-92402.40724, 142017.0021, -24907.23409, -501.9841485, 82970.28312, 17486.61935, 86665.76837, -125242.803, -124956.7945, -38322.74503, -135702.1635]\n",
    "y_coord = [-1809383.515, -1887528.408, -1936401.136, -1747313.691, -1872700.38, -1877801.953, -1759280.809, -1766817.469, -1845740.884, -1839970.706, -1780656.283]\n",
    "\n",
    "lat = [-16.620444] \n",
    "lon = [133.577361]\n",
    "\n",
    "\n",
    "\n",
    "# convert GDA94 to WGS84\n",
    "import pyproj\n",
    "# GDA94 / Geoscience Australia Lambert, EPSG:3112\n",
    "# WGS 84 -- WGS84 - World Geodetic System 1984, used in GPS, EPSG:4326\n",
    "gda94_wgs84 = pyproj.Transformer.from_crs(3112, 4326)\n",
    "for i in range(len(x_coord)):\n",
    "    new_coord = gda94_wgs84.transform(x_coord[i], y_coord[i])\n",
    "    lat.append(new_coord[0])\n",
    "    lon.append(new_coord[1])\n",
    "\n",
    "# replace coordinates in the dataset\n",
    "for i in range(len(wells_no_location)):\n",
    "    beetaloo.loc[beetaloo['WELL_ID'] == wells_no_location[i], 'LATITUDE'] = lat[i]\n",
    "    beetaloo.loc[beetaloo['WELL_ID'] == wells_no_location[i], 'LONGITUDE'] = lon[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a905ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "beetaloo.rename(columns={'LONGITUDE': 'X',\n",
    "                         'LATITUDE' : 'Y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91da72a6-3a69-42e6-9195-e9db28c2ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "beetaloo.drop(['YEAR', 'CALIPER', 'BITSIZE', 'TOC_CORE', 'TOC_CUTTINGS', 'ORDER_3'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace sonic values smaller than 0 for NaN values\n",
    "mask = beetaloo['DT'] < 0\n",
    "beetaloo.loc[mask, 'DT'] = np.nan\n",
    "\n",
    "# replace resistivity values smaller than or equal to 0 for NaN values\n",
    "mask = beetaloo['RES'] <= 0\n",
    "beetaloo.loc[mask, 'RES'] = np.nan\n",
    "\n",
    "# create a new column to store log base 10 of resistivity\n",
    "beetaloo['RES_10'] = np.log10(beetaloo['RES']+1)\n",
    "\n",
    "# replace neutron porosity values smaller than 0 for NaN values\n",
    "mask = beetaloo['NPHI'] < 0\n",
    "beetaloo.loc[mask, 'NPHI'] = np.nan\n",
    "\n",
    "# convert percentage to fraction\n",
    "def convert_neutron(x):\n",
    "    if x >= 1:\n",
    "        return x / 100\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "beetaloo['NPHI'] = beetaloo['NPHI'].apply(convert_neutron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b852f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding for well id\n",
    "well_encoder = LabelEncoder()\n",
    "beetaloo['WELL'] = well_encoder.fit_transform(beetaloo['WELL_ID'])\n",
    "\n",
    "# label encoding for stratigraphy\n",
    "strat_encoder = LabelEncoder()\n",
    "beetaloo['STRAT_ENCODED'] = strat_encoder.fit_transform(beetaloo['STRAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d34f308-da10-4952-9c97-6b2469a88b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WELL_ID', 'Y', 'X', 'DEPTH', 'RHOB', 'GR', 'DT', 'RES', 'SP', 'NPHI',\n",
       "       'LITHO', 'STRAT', 'RES_10', 'WELL', 'STRAT_ENCODED'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns\n",
    "beetaloo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e3010c-55c4-43d3-bfad-761dbff0d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "selected_features = ['WELL', 'X', 'Y', 'DEPTH', \n",
    "                     'RHOB', 'GR', 'DT', 'RES_10', 'SP', 'NPHI',\n",
    "                     'STRAT_ENCODED']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8e4c5-5dec-49a3-a73a-89be0a31082d",
   "metadata": {},
   "source": [
    "## 2. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3065abe7-9c02-43e9-8967-6bb9af33b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of the dataset for modeling\n",
    "data_ml = beetaloo[selected_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466ee954-1ea4-4b3d-9d09-eefbfbd5daf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of data in train set: 0.83\n",
      "Fraction of data in test set: 0.17\n",
      "Total number of samples in dataset: 352143\n"
     ]
    }
   ],
   "source": [
    "# well test selection\n",
    "test_wells = well_encoder.transform(['Tarlee_2', 'Beetaloo_W1', 'Friendship_1', 'Burdo_1', 'McManus_1', 'Amungee_NW1'])\n",
    "\n",
    "# mask for test well\n",
    "test_mask = data_ml['WELL'].isin(test_wells)\n",
    "\n",
    "# column to identify train and test wells\n",
    "data_ml['train_test'] = 'Train'\n",
    "data_ml.loc[test_mask, 'train_test'] = 'Test'\n",
    "\n",
    "# fraction of data\n",
    "train_fraction = data_ml[data_ml['train_test'] == 'Train'].shape[0] / data_ml.shape[0]\n",
    "test_fraction = data_ml[data_ml['train_test'] == 'Test'].shape[0] / data_ml.shape[0]\n",
    "print(f\"Fraction of data in train set: {train_fraction:.2f}\")\n",
    "print(f\"Fraction of data in test set: {test_fraction:.2f}\")\n",
    "print(f\"Total number of samples in dataset: {data_ml.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f52422d5-f455-4635-ac53-8a528c8b55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets\n",
    "train = data_ml[~test_mask].copy()\n",
    "test = data_ml[test_mask].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea21775-9fe1-493f-9447-33a056e668e0",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1975a8-9453-4e4a-b95b-7fa5ef609846",
   "metadata": {},
   "source": [
    "### 3.1. Input Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6dcca82-9c07-4940-908a-458d73685774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of train and test sets\n",
    "X_train = train.copy()\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a8dce30-6a68-4c98-ad86-a90762e2b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features to impute \n",
    "features_mice = ['RHOB', 'GR', 'DT', 'RES_10', 'SP', 'NPHI', 'STRAT_ENCODED']\n",
    "imputed_cols = ['RHOB', 'GR', 'DT', 'RES_10', 'SP', 'NPHI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12967b0b-2ed3-4af7-b7bc-4565c63b12f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Combinations: 119\n"
     ]
    }
   ],
   "source": [
    "# list with all combinations of well-logs for each well\n",
    "unique_wells = X_train['WELL'].unique()\n",
    "combinations = []\n",
    "for well in unique_wells:\n",
    "    for feature in imputed_cols:\n",
    "        # only for well-logs that are not completely NaN\n",
    "        if not X_train.loc[X_train['WELL'] == well, feature].isna().all():\n",
    "            combinations.append((well, feature))\n",
    "            \n",
    "print('Number of Combinations:', len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969c550d-504c-43d6-a06d-e1885491963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to impute NaN values using iterative imputer (MICE)\n",
    "def impute(train_data, cols_imp, model):\n",
    "    mice = IterativeImputer(estimator=model, initial_strategy='mean' , random_state=17, max_iter=20, tol=0.01)\n",
    "    mice.fit(train_data[cols_imp])\n",
    "    imputed_train = mice.transform(train_data[cols_imp])\n",
    "    return imputed_train, mice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827b83f",
   "metadata": {},
   "source": [
    "### 3.2. Model Training with the best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d543e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(X_train, model, param_grid, well_logs, cols_imp, n_splits, n_jobs):\n",
    "    \"\"\"\n",
    "    Training Model with MICE\n",
    "    \n",
    "    Parameters\n",
    "    ----------------------------------------------------------------------------------\n",
    "        X_train: (pd.DataFrame) \n",
    "            Training data\n",
    "        \n",
    "        model: (model object for imputation)\n",
    "            Model (e.g., KNeighborsRegressor, BayesianRidge, RF, XGBoost)\n",
    "            \n",
    "        param_grid: (dict)\n",
    "            Dictionary of hyperparameters for the model\n",
    "            \n",
    "        well_logs: (list)\n",
    "            List of well logs to evaluate\n",
    "            \n",
    "        cols_imp: (list)\n",
    "            List of columns to impute\n",
    "            \n",
    "        n_splits: (int)\n",
    "            Number of cross-validation splits\n",
    "            \n",
    "        n_jobs: (int)\n",
    "            Number of jobs to run in parallel during imputation\n",
    "    \n",
    "    Returns\n",
    "    ----------------------------------------------------------------------------------\n",
    "        scaler: (object)\n",
    "            MinMaxScaler object fitted on the imputed training data\n",
    "            \n",
    "        imp_model: (object)\n",
    "            Trained imputation model\n",
    "    \"\"\"\n",
    "    # copy of the data to work with\n",
    "    data_train = X_train.copy()\n",
    "\n",
    "    # scale the data for training\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data_train[cols_imp])\n",
    "    X_training_scaled = scaler.transform(data_train[cols_imp])\n",
    "    X_training_scaled_df = pd.DataFrame(X_training_scaled, \n",
    "                                        columns=cols_imp,\n",
    "                                        index=X_train.index)\n",
    "\n",
    "    # impute NaN values using iterative imputer\n",
    "    if model == KNeighborsRegressor:\n",
    "        X_training_imp, imp_model = impute(train_data= X_training_scaled_df,  \n",
    "                                           cols_imp=cols_imp,\n",
    "                                           model=model(**param_grid, n_jobs=n_jobs))\n",
    "\n",
    "    elif model == BayesianRidge:\n",
    "        X_training_imp, imp_model = impute(train_data=X_training_scaled_df,\n",
    "                                           cols_imp=cols_imp,\n",
    "                                           model=model(**param_grid))\n",
    "\n",
    "    else:\n",
    "        X_training_imp, imp_model = impute(train_data= X_training_scaled_df,  \n",
    "                                           cols_imp=cols_imp,\n",
    "                                           model=model(**param_grid, random_state=17, n_jobs=n_jobs))\n",
    "\n",
    "    return scaler, imp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20902d7c",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "383132f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_knr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a5cabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcbae\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler_knr, imp_model_knr = training_model(X_train=X_train, \n",
    "                                           model=KNeighborsRegressor, \n",
    "                                           param_grid=param_grid_knr,\n",
    "                                           well_logs=imputed_cols,\n",
    "                                           cols_imp=features_mice, \n",
    "                                           n_splits=5,\n",
    "                                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bccf93",
   "metadata": {},
   "source": [
    "### BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "406c3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_br = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33608c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_br, imp_model_br = training_model(X_train=X_train, \n",
    "                                         model=BayesianRidge, \n",
    "                                         param_grid=param_grid_br,\n",
    "                                         well_logs=imputed_cols,\n",
    "                                         cols_imp=features_mice, \n",
    "                                         n_splits=5,\n",
    "                                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7ce9a",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beeff7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff4b79a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcbae\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler_rf, imp_model_rf = training_model(X_train=X_train, \n",
    "                                         model=RandomForestRegressor, \n",
    "                                         param_grid=param_grid_rf,\n",
    "                                         well_logs=imputed_cols,\n",
    "                                         cols_imp=features_mice, \n",
    "                                         n_splits=5,\n",
    "                                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41c52b",
   "metadata": {},
   "source": [
    "### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd80b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {'max_depth': 6, 'reg_lambda': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "410064fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcbae\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler_xgb, imp_model_xgb = training_model(X_train=X_train, \n",
    "                                           model=XGBRegressor, \n",
    "                                           param_grid=param_grid_xgb,\n",
    "                                           well_logs=imputed_cols,\n",
    "                                           cols_imp=features_mice, \n",
    "                                           n_splits=5,\n",
    "                                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f84364",
   "metadata": {},
   "source": [
    "## 4. Testing Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b260a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(X_test, scaler, imp_model, well_logs, cols_imp, n_splits):\n",
    "    \"\"\"\n",
    "    Impute Test Data using Train Model\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------------------------------------------------\n",
    "        X_test: (pd.DataFrame) \n",
    "            Test data\n",
    "        \n",
    "        scaler: (scaler object)\n",
    "            Scaler object fitted on the training data\n",
    "            \n",
    "        imp_model: (imputation model object)\n",
    "            Imputation model trained on the training data\n",
    "            \n",
    "        well_logs: (list)\n",
    "            List of well logs to evaluate\n",
    "            \n",
    "        cols_imp: (list)\n",
    "            List of columns to impute\n",
    "            \n",
    "        n_splits: (int)\n",
    "            Number of cross-validation splits\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------------------------------------------------\n",
    "        combined_df: (pd.DataFrame)\n",
    "            Combined results dataframe containing original scaled values, \n",
    "            scaled imputed values, and imputed values for each well log\n",
    "    \"\"\"\n",
    "    # scale the original test data\n",
    "    X_test_original_scale = scaler.transform(X_test[cols_imp])        \n",
    "    X_test_original_scale_df = pd.DataFrame(X_test_original_scale,\n",
    "                                      columns=cols_imp,\n",
    "                                      index=X_test.index)\n",
    "    X_test_original_scale_df['WELL'] = X_test['WELL']\n",
    "    \n",
    "    unique_wells = X_test['WELL'].unique()\n",
    "    combinations = []\n",
    "    for well in unique_wells:\n",
    "        for feature in imputed_cols:\n",
    "            # only for well-logs that are not completely NaN\n",
    "            if not X_test.loc[X_test['WELL'] == well, feature].isna().all():\n",
    "                combinations.append((well, feature))\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, random_state=17, shuffle=True)\n",
    "    \n",
    "    X_test_sc_imp_result = X_test.copy()\n",
    "    X_test_imp_result = X_test.copy()\n",
    "     \n",
    "    for i, (test_index, val_index) in enumerate(kf.split(combinations)):\n",
    "\n",
    "        # test and validations sets\n",
    "        test_combinations = [combinations[i] for i in test_index]\n",
    "        validation_combinations = [combinations[i] for i in val_index]\n",
    "\n",
    "        # copy of the data to work with\n",
    "        data_test = X_test.copy()\n",
    "        \n",
    "        # set values to NaN in the data to impute using the validation combinations\n",
    "        for well_id, feature_name in validation_combinations:\n",
    "            data_test.loc[data_test['WELL']==well_id, feature_name] = np.nan\n",
    "\n",
    "\n",
    "        # scale the test with NaN using the scaler object fitted on the training data\n",
    "        X_test_scaled = scaler.transform(data_test[cols_imp])\n",
    "        X_test_scaled_df = pd.DataFrame(X_test_scaled, \n",
    "                                        columns=cols_imp,\n",
    "                                        index=X_test.index)\n",
    "\n",
    "        # impute NaN values in the test data using the imp_model trained on the training data\n",
    "        X_test_imp = imp_model.transform(X_test_scaled_df[cols_imp])\n",
    "        X_test_imp_scaled = pd.DataFrame(X_test_imp, columns=cols_imp, index=X_test.index)\n",
    "\n",
    "        \n",
    "        # inverse transform the imputed values\n",
    "        X_test_imp_unscaled = scaler.inverse_transform(X_test_imp_scaled)\n",
    "        X_test_imp_df = pd.DataFrame(X_test_imp_unscaled, \n",
    "                                     columns=cols_imp, \n",
    "                                     index=X_test.index)\n",
    "        \n",
    "        \n",
    "        # store results of the scaled imputation using validation combinations\n",
    "        for well_id, feature_name in validation_combinations:\n",
    "            scaled_imputed_result = X_test_imp_scaled.loc[X_test['WELL']==well_id, feature_name]\n",
    "            X_test_sc_imp_result.loc[X_test['WELL']==well_id, feature_name] = scaled_imputed_result\n",
    "        \n",
    "        # store results of the imputation using validation combinations\n",
    "        for well_id, feature_name in validation_combinations:\n",
    "            imputed_result = X_test_imp_df.loc[X_test['WELL']==well_id, feature_name]\n",
    "            X_test_imp_result.loc[X_test['WELL']==well_id, feature_name] = imputed_result\n",
    "                \n",
    "    # rename columns\n",
    "    X_test_original_scale_df.rename(columns=lambda x: x + '_SCALED', inplace=True)\n",
    "    X_test_sc_imp_result.rename(columns=lambda x: x + '_IMP_SCALED', inplace=True)\n",
    "    X_test_imp_result.rename(columns=lambda x: x + '_IMP', inplace=True)\n",
    "    \n",
    "    # combine dataframes\n",
    "    combined_df = pd.concat([X_test_original_scale_df[[column + '_SCALED' for column in well_logs]],\n",
    "                             X_test_sc_imp_result[[column + '_IMP_SCALED' for column in well_logs]],\n",
    "                             X_test_imp_result[[column + '_IMP' for column in well_logs]]\n",
    "                            ], axis=1)\n",
    "\n",
    "    return  combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cc7d1",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86acf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_knr = testing_model(X_test=X_test, \n",
    "                                scaler=scaler_knr, \n",
    "                                imp_model=imp_model_knr,\n",
    "                                well_logs=imputed_cols,\n",
    "                                cols_imp=features_mice,\n",
    "                                n_splits=5\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec24e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_knr.to_csv('test_result_knr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a4138",
   "metadata": {},
   "source": [
    "### BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "385e011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_br = testing_model(X_test=X_test, \n",
    "                               scaler=scaler_br, \n",
    "                               imp_model=imp_model_br,\n",
    "                               well_logs=imputed_cols,\n",
    "                               cols_imp=features_mice,\n",
    "                               n_splits=5\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9745b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_br.to_csv('test_result_br.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0269a74",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b09e84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_rf = testing_model(X_test=X_test, \n",
    "                               scaler=scaler_rf, \n",
    "                               imp_model=imp_model_rf,\n",
    "                               well_logs=imputed_cols,\n",
    "                               cols_imp=features_mice,\n",
    "                               n_splits=5\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93f2f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_rf.to_csv('test_result_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc63f4",
   "metadata": {},
   "source": [
    "### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da1819a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_xgb = testing_model(X_test=X_test, \n",
    "                                scaler=scaler_xgb, \n",
    "                                imp_model=imp_model_xgb,\n",
    "                                well_logs=imputed_cols,\n",
    "                                cols_imp=features_mice,\n",
    "                                n_splits=5\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "495794f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_xgb.to_csv('test_result_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b8157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
