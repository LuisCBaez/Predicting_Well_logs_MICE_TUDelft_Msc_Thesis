{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43289b8a",
   "metadata": {},
   "source": [
    "# Force-200: MICE (max_iter=20), EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47baab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a8b5d",
   "metadata": {},
   "source": [
    "# 1. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534b988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# force train dataset\n",
    "train = pd.read_csv(filepath_or_buffer='force_train.csv', sep=';')\n",
    "\n",
    "# force test dataset\n",
    "test = pd.read_csv(filepath_or_buffer='force_test.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839ca7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "train.rename(columns={'WELL': 'WELL_ID', \n",
    "                      'DEPTH_MD': 'DEPTH',\n",
    "                      'X_LOC' : 'X',\n",
    "                      'Y_LOC': 'Y', \n",
    "                      'GROUP': 'STRAT'\n",
    "                     }, inplace=True\n",
    "            )\n",
    "\n",
    "test.rename(columns={'WELL': 'WELL_ID', \n",
    "                     'DEPTH_MD': 'DEPTH',\n",
    "                     'X_LOC' : 'X',\n",
    "                     'Y_LOC': 'Y', \n",
    "                     'GROUP': 'STRAT'\n",
    "                    }, inplace=True\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91da72a6-3a69-42e6-9195-e9db28c2ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "train.drop(['Z_LOC', 'CALI', 'SGR', 'BS', 'ROP', \n",
    "            'DCAL', 'MUDWEIGHT', 'RMIC','ROPA', 'RXO', \n",
    "            'FORCE_2020_LITHOFACIES_LITHOLOGY',\n",
    "            'FORCE_2020_LITHOFACIES_CONFIDENCE'], axis=1, inplace=True)\n",
    "\n",
    "test.drop(['Z_LOC', 'CALI', 'SGR', 'BS', 'ROP', \n",
    "            'DCAL', 'MUDWEIGHT', 'RMIC','ROPA', 'RXO', \n",
    "            'FORCE_2020_LITHOFACIES_LITHOLOGY',\n",
    "            'FORCE_2020_LITHOFACIES_CONFIDENCE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column to store log base 10 of resistivity\n",
    "# train\n",
    "train['RD10'] = np.log10(train['RDEP']+1)\n",
    "train['RM10'] = np.log10(train['RMED']+1)\n",
    "train['RS10'] = np.log10(train['RSHA']+1)\n",
    "\n",
    "# test\n",
    "test['RD10'] = np.log10(test['RDEP']+1)\n",
    "test['RM10'] = np.log10(test['RMED']+1)\n",
    "test['RS10'] = np.log10(test['RSHA']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b852f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding for well id\n",
    "well_encoder = LabelEncoder()\n",
    "well_encoder.fit(pd.concat([train, test]).WELL_ID.replace(np.nan, ''))\n",
    "\n",
    "train['WELL'] = well_encoder.transform(train.WELL_ID.replace(np.nan, ''))\n",
    "train['WELL'] = train['WELL'].astype(int)\n",
    "\n",
    "test['WELL'] = well_encoder.transform(test.WELL_ID.replace(np.nan, ''))\n",
    "test['WELL'] = test['WELL'].astype(int)\n",
    "\n",
    "\n",
    "# label encoding for stratigraphy\n",
    "group_encoder = LabelEncoder()\n",
    "group_encoder.fit(pd.concat([train, test]).STRAT.replace(np.nan, ''))\n",
    "\n",
    "train['STRAT_ENCODED'] = group_encoder.transform(train.STRAT.replace(np.nan, ''))\n",
    "train['STRAT_ENCODED'] = train['STRAT_ENCODED'].astype(int)\n",
    "\n",
    "test['STRAT_ENCODED'] = group_encoder.transform(test.STRAT.replace(np.nan, ''))\n",
    "test['STRAT_ENCODED'] = test['STRAT_ENCODED'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d34f308-da10-4952-9c97-6b2469a88b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WELL_ID', 'DEPTH', 'X', 'Y', 'STRAT', 'FORMATION', 'RSHA', 'RMED',\n",
       "       'RDEP', 'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'DTS', 'DRHO', 'RD10',\n",
       "       'RM10', 'RS10', 'WELL', 'STRAT_ENCODED'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e3010c-55c4-43d3-bfad-761dbff0d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "selected_features = ['WELL', 'X_LOC', 'Y_LOC', 'DEPTH',\n",
    "                     'RHOB', 'GR', 'DTC', 'DTS',\n",
    "                     'RD10', 'RM10', 'RS10',\n",
    "                     'SP', 'NPHI','PEF',\n",
    "                     'STRAT_ENCODED']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea21775-9fe1-493f-9447-33a056e668e0",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d3140",
   "metadata": {},
   "source": [
    "### 3.1. Input Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6dcca82-9c07-4940-908a-458d73685774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy of train and test sets\n",
    "X_train = train.copy()\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a8dce30-6a68-4c98-ad86-a90762e2b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features to impute \n",
    "features_mice = ['RHOB', 'GR', 'DTC', 'DTS', 'RD10', 'RM10', 'RS10', 'SP', 'NPHI', 'PEF', 'STRAT_ENCODED']\n",
    "imputed_cols = ['RHOB', 'GR', 'DTC', 'DTS', 'RD10', 'RM10', 'RS10', 'SP', 'NPHI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c892a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "well_logs = imputed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12967b0b-2ed3-4af7-b7bc-4565c63b12f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Combinations: 761\n"
     ]
    }
   ],
   "source": [
    "# list with all combinations of well-logs for each well\n",
    "unique_wells = X_train['WELL'].unique()\n",
    "combinations = []\n",
    "for well in unique_wells:\n",
    "    for feature in imputed_cols:\n",
    "        # only for well-logs that are not completely NaN\n",
    "        if not X_train.loc[X_train['WELL'] == well, feature].isna().all():\n",
    "            combinations.append((well, feature))\n",
    "            \n",
    "print('Number of Combinations:', len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "969c550d-504c-43d6-a06d-e1885491963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to impute NaN values using iterative imputer (MICE)\n",
    "def impute(train_data, cols_imp, model):\n",
    "    mice = IterativeImputer(estimator=model, initial_strategy='mean' , random_state=17, max_iter=20, tol=0.01)\n",
    "    mice.fit(train_data[cols_imp])\n",
    "    imputed_train = mice.transform(train_data[cols_imp])\n",
    "    return imputed_train, mice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827b83f",
   "metadata": {},
   "source": [
    "### 3.2. Model Training with the best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d543e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(X_train, model, param_grid, well_logs, cols_imp, n_splits, n_jobs):\n",
    "    \"\"\"\n",
    "    Training Model with MICE\n",
    "    \n",
    "    Parameters\n",
    "    ----------------------------------------------------------------------------------\n",
    "        X_train: (pd.DataFrame) \n",
    "            Training data\n",
    "        \n",
    "        model: (model object for imputation)\n",
    "            Model (e.g., KNeighborsRegressor, BayesianRidge, RF, XGBoost)\n",
    "            \n",
    "        param_grid: (dict)\n",
    "            Dictionary of hyperparameters for the model\n",
    "            \n",
    "        well_logs: (list)\n",
    "            List of well logs to evaluate\n",
    "            \n",
    "        cols_imp: (list)\n",
    "            List of columns to impute\n",
    "            \n",
    "        n_splits: (int)\n",
    "            Number of cross-validation splits\n",
    "            \n",
    "        n_jobs: (int)\n",
    "            Number of jobs to run in parallel during imputation\n",
    "    \n",
    "    Returns\n",
    "    ----------------------------------------------------------------------------------\n",
    "        scaler: (object)\n",
    "            MinMaxScaler object fitted on the imputed training data\n",
    "            \n",
    "        imp_model: (object)\n",
    "            Trained imputation model\n",
    "    \"\"\"\n",
    "    # copy of the data to work with\n",
    "    data_train = X_train.copy()\n",
    "\n",
    "    # scale the data for training\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data_train[cols_imp])\n",
    "    X_training_scaled = scaler.transform(data_train[cols_imp])\n",
    "    X_training_scaled_df = pd.DataFrame(X_training_scaled, \n",
    "                                        columns=cols_imp,\n",
    "                                        index=X_train.index)\n",
    "\n",
    "    # impute NaN values using iterative imputer\n",
    "    if model == KNeighborsRegressor:\n",
    "        X_training_imp, imp_model = impute(train_data= X_training_scaled_df,  \n",
    "                                           cols_imp=cols_imp,\n",
    "                                           model=model(**param_grid, n_jobs=n_jobs))\n",
    "\n",
    "    elif model == BayesianRidge:\n",
    "        X_training_imp, imp_model = impute(train_data=X_training_scaled_df,\n",
    "                                           cols_imp=cols_imp,\n",
    "                                           model=model(**param_grid))\n",
    "\n",
    "    else:\n",
    "        X_training_imp, imp_model = impute(train_data= X_training_scaled_df,  \n",
    "                                           cols_imp=cols_imp,\n",
    "                                           model=model(**param_grid, random_state=17, n_jobs=n_jobs))\n",
    "\n",
    "    return scaler, imp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20902d7c",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "383132f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_knr = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a5cabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcbae\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler_knr, imp_model_knr = training_model(X_train=X_train, \n",
    "                                           model=KNeighborsRegressor, \n",
    "                                           param_grid=param_grid_knr,\n",
    "                                           well_logs=imputed_cols,\n",
    "                                           cols_imp=features_mice, \n",
    "                                           n_splits=5,\n",
    "                                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bccf93",
   "metadata": {},
   "source": [
    "### BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406c3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_br = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33608c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcbae\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler_br, imp_model_br = training_model(X_train=X_train, \n",
    "                                         model=BayesianRidge, \n",
    "                                         param_grid=param_grid_br,\n",
    "                                         well_logs=imputed_cols,\n",
    "                                         cols_imp=features_mice, \n",
    "                                         n_splits=5,\n",
    "                                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7ce9a",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beeff7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {'max_depth': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff4b79a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcbae\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler_rf, imp_model_rf = training_model(X_train=X_train, \n",
    "                                         model=RandomForestRegressor, \n",
    "                                         param_grid=param_grid_rf,\n",
    "                                         well_logs=imputed_cols,\n",
    "                                         cols_imp=features_mice, \n",
    "                                         n_splits=5,\n",
    "                                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41c52b",
   "metadata": {},
   "source": [
    "### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd80b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "410064fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcbae\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler_xgb, imp_model_xgb = training_model(X_train=X_train, \n",
    "                                           model=XGBRegressor, \n",
    "                                           param_grid=param_grid_xgb,\n",
    "                                           well_logs=imputed_cols,\n",
    "                                           cols_imp=features_mice, \n",
    "                                           n_splits=5,\n",
    "                                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f84364",
   "metadata": {},
   "source": [
    "## 4. Testing Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b260a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(X_test, scaler, imp_model, well_logs, cols_imp, n_splits):\n",
    "    \"\"\"\n",
    "    Impute Test Data using Train Model\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------------------------------------------------\n",
    "        X_test: (pd.DataFrame) \n",
    "            Test data\n",
    "        \n",
    "        scaler: (scaler object)\n",
    "            Scaler object fitted on the training data\n",
    "            \n",
    "        imp_model: (imputation model object)\n",
    "            Imputation model trained on the training data\n",
    "            \n",
    "        well_logs: (list)\n",
    "            List of well logs to evaluate\n",
    "            \n",
    "        cols_imp: (list)\n",
    "            List of columns to impute\n",
    "            \n",
    "        n_splits: (int)\n",
    "            Number of cross-validation splits\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------------------------------------------------\n",
    "        combined_df: (pd.DataFrame)\n",
    "            Combined results dataframe containing original scaled values, \n",
    "            scaled imputed values, and imputed values for each well log\n",
    "    \"\"\"\n",
    "    # scale the original test data\n",
    "    X_test_original_scale = scaler.transform(X_test[cols_imp])        \n",
    "    X_test_original_scale_df = pd.DataFrame(X_test_original_scale,\n",
    "                                      columns=cols_imp,\n",
    "                                      index=X_test.index)\n",
    "    X_test_original_scale_df['WELL'] = X_test['WELL']\n",
    "    \n",
    "    unique_wells = X_test['WELL'].unique()\n",
    "    combinations = []\n",
    "    for well in unique_wells:\n",
    "        for feature in imputed_cols:\n",
    "            # only for well-logs that are not completely NaN\n",
    "            if not X_test.loc[X_test['WELL'] == well, feature].isna().all():\n",
    "                combinations.append((well, feature))\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, random_state=17, shuffle=True)\n",
    "    \n",
    "    X_test_sc_imp_result = X_test.copy()\n",
    "    X_test_imp_result = X_test.copy()\n",
    "     \n",
    "    for i, (test_index, val_index) in enumerate(kf.split(combinations)):\n",
    "\n",
    "        # test and validations sets\n",
    "        test_combinations = [combinations[i] for i in test_index]\n",
    "        validation_combinations = [combinations[i] for i in val_index]\n",
    "\n",
    "        # copy of the data to work with\n",
    "        data_test = X_test.copy()\n",
    "        \n",
    "        # set values to NaN in the data to impute using the validation combinations\n",
    "        for well_id, feature_name in validation_combinations:\n",
    "            data_test.loc[data_test['WELL']==well_id, feature_name] = np.nan\n",
    "\n",
    "\n",
    "        # scale the test with NaN using the scaler object fitted on the training data\n",
    "        X_test_scaled = scaler.transform(data_test[cols_imp])\n",
    "        X_test_scaled_df = pd.DataFrame(X_test_scaled, \n",
    "                                        columns=cols_imp,\n",
    "                                        index=X_test.index)\n",
    "\n",
    "        # impute NaN values in the test data using the imp_model trained on the training data\n",
    "        X_test_imp = imp_model.transform(X_test_scaled_df[cols_imp])\n",
    "        X_test_imp_scaled = pd.DataFrame(X_test_imp, columns=cols_imp, index=X_test.index)\n",
    "\n",
    "        \n",
    "        # inverse transform the imputed values\n",
    "        X_test_imp_unscaled = scaler.inverse_transform(X_test_imp_scaled)\n",
    "        X_test_imp_df = pd.DataFrame(X_test_imp_unscaled, \n",
    "                                     columns=cols_imp, \n",
    "                                     index=X_test.index)\n",
    "        \n",
    "        \n",
    "        # store results of the scaled imputation using validation combinations\n",
    "        for well_id, feature_name in validation_combinations:\n",
    "            scaled_imputed_result = X_test_imp_scaled.loc[X_test['WELL']==well_id, feature_name]\n",
    "            X_test_sc_imp_result.loc[X_test['WELL']==well_id, feature_name] = scaled_imputed_result\n",
    "        \n",
    "        # store results of the imputation using validation combinations\n",
    "        for well_id, feature_name in validation_combinations:\n",
    "            imputed_result = X_test_imp_df.loc[X_test['WELL']==well_id, feature_name]\n",
    "            X_test_imp_result.loc[X_test['WELL']==well_id, feature_name] = imputed_result\n",
    "                \n",
    "    # rename columns\n",
    "    X_test_original_scale_df.rename(columns=lambda x: x + '_SCALED', inplace=True)\n",
    "    X_test_sc_imp_result.rename(columns=lambda x: x + '_IMP_SCALED', inplace=True)\n",
    "    X_test_imp_result.rename(columns=lambda x: x + '_IMP', inplace=True)\n",
    "    \n",
    "    # combine dataframes\n",
    "    combined_df = pd.concat([X_test_original_scale_df[[column + '_SCALED' for column in well_logs]],\n",
    "                             X_test_sc_imp_result[[column + '_IMP_SCALED' for column in well_logs]],\n",
    "                             X_test_imp_result[[column + '_IMP' for column in well_logs]]\n",
    "                            ], axis=1)\n",
    "\n",
    "    return  combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cc7d1",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86acf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_knr = testing_model(X_test=X_test, \n",
    "                                scaler=scaler_knr, \n",
    "                                imp_model=imp_model_knr,\n",
    "                                well_logs=imputed_cols,\n",
    "                                cols_imp=features_mice,\n",
    "                                n_splits=5\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec24e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_knr.to_csv('test_result_knr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a4138",
   "metadata": {},
   "source": [
    "### BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "385e011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_br = testing_model(X_test=X_test, \n",
    "                               scaler=scaler_br, \n",
    "                               imp_model=imp_model_br,\n",
    "                               well_logs=imputed_cols,\n",
    "                               cols_imp=features_mice,\n",
    "                               n_splits=5\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9745b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_br.to_csv('test_result_br.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0269a74",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b09e84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_rf = testing_model(X_test=X_test, \n",
    "                               scaler=scaler_rf, \n",
    "                               imp_model=imp_model_rf,\n",
    "                               well_logs=imputed_cols,\n",
    "                               cols_imp=features_mice,\n",
    "                               n_splits=5\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93f2f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_rf.to_csv('test_result_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dc63f4",
   "metadata": {},
   "source": [
    "### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da1819a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_xgb = testing_model(X_test=X_test, \n",
    "                                scaler=scaler_xgb, \n",
    "                                imp_model=imp_model_xgb,\n",
    "                                well_logs=imputed_cols,\n",
    "                                cols_imp=features_mice,\n",
    "                                n_splits=5\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "495794f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv file\n",
    "test_result_xgb.to_csv('test_result_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b8157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16a907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c78bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
